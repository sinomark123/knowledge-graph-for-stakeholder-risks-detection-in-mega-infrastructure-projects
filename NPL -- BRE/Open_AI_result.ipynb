{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai=pd.read_csv(r\"OPENAI_LLM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['risks description', 'risk_type', 'keywords', 'sentence tags'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    operational, traffic, drought, political, cons...\n",
       "1    Tolerance-related risks, safety risks, crash r...\n",
       "2         financial, safety, structural, environmental\n",
       "3    Safety, Catastrophic, Political and Legal, Inc...\n",
       "4    Payment delays, project delays, transmission r...\n",
       "Name: risk_type, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.risk_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=TextLoader(r\"prj_abstract.txt\")\n",
    "text=loader.load()\n",
    "text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs=text_splitter.split_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_insertion(words: str):\n",
    "    return f\"\"\"help me to identify and extract organizations in the ' {words} '? And if there are stakeholder in sentence act as organization, please mark it out and specify the type . Also pick keywords to label what ENTIRE sentence talked about from given tags: [environment science, environment studies, engineering civil, engineering industrial, management, engineering manufacturing, constructing building, constructing technology, transportation, finance]. If you cannot judge the organizations or there is no matched tag to label the entire sentence, please return 'None'. Show organization, keyword and tags appeared in sentence separately. Attention, you can only predict the category in above given criteria and no more than 15 words.\n",
    "\"\"\"\n",
    "# among [governments, design institutes, investment banking, contractors, consultants, subcontractors, suppliers, media, political party, environmental groups]. If it is an individual, please specify the type among [project managers, architects, engineers, surveyors, planners, workers, general public, journalists, politicians, or researchers]\n",
    "# result=llm(stk_template)\n",
    "# prompt_risk=PromptTemplate(template=stk_template, input_variables=[\"sentence\"])\n",
    "# prompt_risk.format(sentence=words)\n",
    "# chain = LLMChain(llm=ChatOpenAI(), prompt=prompt_risk)\n",
    "# chainres=chain.run(words)\n",
    "# mask=chainres.split(\"\\n\")\n",
    "# print(mask)\n",
    "# res=llm(template_insertion(words))\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_template(words: str):\n",
    "    return f\"\"\"help me to identify and extract risks in the '{words}'? please mark it out and specify the type . Also pick keywords to label what ENTIRE sentence talked about from given tags: [environment science, environment studies, engineering civil, engineering industrial, management, engineering manufacturing, constructing building, constructing technology, transportation, finance]. If you cannot judge the organizations or there is no matched tag to label the entire sentence, please return 'None'. Show risks, type of risks, keyword and tags appeared in sentence separately. Attention, you can only predict the output in above given criteria and no more than 15 words.No need to specific mark out on every sentence\n",
    "\n",
    "    Answer example: \"\n",
    "    risks: Construction cost overrun, Ship collision and grounding \\n\n",
    "\n",
    "    risk_type : Financial, Technical \\n\n",
    "\n",
    "    keywords : Risk Management \\n\n",
    "\n",
    "    tags : Environment Science, Environment Studies, Finance\" \\n\n",
    "    Answer no more than 20 words.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def risk_template_backwords(words: str):\n",
    "    return f\"\"\"help me to identify and extract risks in the '{words}'? please mark it out and specify the type . Also pick keywords to label what ENTIRE sentence talked about from given tags: [environment science, environment studies, engineering civil, engineering industrial, management, engineering manufacturing, constructing building, constructing technology, transportation, finance]. If you cannot judge the organizations or there is no matched tag to label the entire sentence, please return 'None'. Show type of risk follows its risks, keyword and tags appeared in sentence separately. Attention, you can only predict the category in above given criteria and no more than 20 words. No need to specific mark out on every sentence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example, template_example=stake.iloc[0:2, ], pd.DataFrame([], columns=[\"source\", \"org/stk\", \"keyword\", \"tags\"])\n",
    "logging.basicConfig(filename=\"multithreading.log\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df=stake, chunk_size=12)->list:\n",
    "    chunk, length=[], df.size\n",
    "    for val in range(0, length, chunk_size): chunk.append((\" \".join(df.iloc[val: val+chunk_size].Abstract.to_list()), val))\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stk_keyword_split(res: list[str]):\n",
    "    \"\"\"res receive value from original feedback, right after OpenAI API return the value and split with \\n\"\"\"\n",
    "    feedback=[val.split(\": \")[1] for val in res if val!='']\n",
    "    if len(feedback)>3: feedback=[\", \".join([feedback[0]+feedback[1]]), *feedback[2:]]\n",
    "    elif len(feedback)<3: feedback=[*feedback, *[None]*(3-(len(feedback)))]\n",
    "    return feedback\n",
    "\n",
    "def risk_keyword_split(res: str):\n",
    "    res=res.split(\"\\n\")\n",
    "    feedback=[val.split(\": \")[1] for val in res if val!='']\n",
    "    if len(feedback)<4: feedback=[*feedback, *([None]*(4-len(feedback)))]\n",
    "    return feedback\n",
    "\n",
    "llm=OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_df=pd.DataFrame([], columns=[\"risks description\", \"risk_type\", \"keywords\", \"sentence tags\", \"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_length, chunk_size=len(risk), 10\n",
    "for times in range(2594, risk_length, chunk_size):\n",
    "    long_phrase=risk.Abstract[times: times+chunk_size].to_list()\n",
    "    words=risk_template(\" \".join(long_phrase))\n",
    "    try:\n",
    "        response=llm(words)\n",
    "        risk_df.loc[len(risk_df)]=risk_keyword_split(response)+[long_phrase]\n",
    "    except Exception as e:\n",
    "        print(f\"the possible reason is {e}, be reminded, processed risk up to {times}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_phrase=risk.Abstract[1534: 1544].to_list()  # 176-181, 181--190\n",
    "# words=risk_template(\" \".join(long_phrase))\n",
    "# response=llm(words)\n",
    "# risk_df.loc[len(risk_df)]=risk_keyword_split(response)\n",
    "apprea=pd.DataFrame(data=[], columns=[\"Source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in range(156, 177, 10):\n",
    "    phrases=\" \".join(risk.Abstract[val: val+10].to_list())\n",
    "    apprea.loc[len(apprea)]=phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(long_words: tuple):\n",
    "    template=template_insertion(long_words[0])\n",
    "    try:\n",
    "        feedback=llm(template).split(\"\\n\")\n",
    "        integration=stk_keyword_split(feedback)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing row {long_words[1]}: {e}\")\n",
    "        sys.exit(\"Warning\")\n",
    "    # print(feedback)\n",
    "    return [words, *integration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches=split_dataframe(stake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=[]\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     futures=executor.map(tagger, batches)\n",
    "#\n",
    "#     for future in futures:\n",
    "#         results+=future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=[val[0] for val in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_doc=[\" \".join(stake.Abstract[val:val+2].to_list()) for val in range(0, 10, 2)]\n",
    "temp_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=DirectoryLoader(\"D:/Code Working Area/Jupyter/knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects/ExcelData/docu/\", glob=\"**/*.txt\")\n",
    "document=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts=text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "dosearch=Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=dosearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "\n",
    "# let's make sure to not retry on an invalid request, because that is what we want to demonstrate\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.InvalidRequestError))\n",
    "def get_embedding(text_or_tokens, model=EMBEDDING_MODEL):\n",
    "    return openai.Embedding.create(input=text_or_tokens, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=words[0:100]\n",
    "try:\n",
    "    values=get_embedding(text)\n",
    "except openai.InvalidRequestError as e:\n",
    "    print(e)\n",
    "# [\"data\"][0][\"embedding\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
