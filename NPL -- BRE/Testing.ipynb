{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa5fb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORG'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "import numpy\n",
    "from spacy import displacy\n",
    "from fpgrowth_py import fpgrowth\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "doc.ents[0].label_\n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5509779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', 0, 13, 'GPE')]\n",
      "['San', 'B', 'GPE']\n",
      "['Francisco', 'I', 'GPE']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n",
    "# token level\n",
    "ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\n",
    "ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\n",
    "print(ent_san)  # ['San', 'B', 'GPE']\n",
    "print(ent_francisco)  # ['Francisco', 'I', 'GPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df181db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before []\n",
      "After [('fb', 0, 1, 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"fb is hiring a new vice president of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)\n",
    "# The model didn't recognize \"fb\" as an entity :(\n",
    "\n",
    "# Create a span for the new entity\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc.ents)\n",
    "\n",
    "# Option 1: Modify the provided entity spans, leaving the rest unmodified\n",
    "doc.set_ents([fb_ent], default=\"unmodified\")\n",
    "\n",
    "# Option 2: Assign a complete list of ents to doc.ents\n",
    "doc.ents = orig_ents + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\n",
    "print('After', ents)\n",
    "# [('fb', 0, 1, 'ORG')] ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2abae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ()\n",
      "After (London,)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.make_doc(\"London is a big city in the United Kingdom.\")\n",
    "print(\"Before\", doc.ents)  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = numpy.zeros((len(doc), len(header)), dtype=\"uint64\")\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[\"GPE\"]\n",
    "doc.from_array(header, attr_array)\n",
    "print(\"After\", doc.ents)  # [London]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00073614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sebastian Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " started working on self-driving cars at the \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HKPU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", few people outside of the company took him seriously.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"When Sebastian Thrun started working on self-driving cars at the HKPU in 2007, few people outside of the company took him seriously.\"\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81b77676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ee834e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\n",
    "nlp.enable_pipe(\"senter\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26385017",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsfile = r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\Transactions.json\"\n",
    "\n",
    "project = r\"D:\\Study\\Real Estate\\Project\\Gg.csv\"\n",
    "risk = r\"D:\\Study\\Real Estate\\Risk\\Risk New\\Risk_Simplified.xlsx\"\n",
    "stake = r\"D:\\Study\\Real Estate\\StakeHolder\\expansive.csv\"\n",
    "\n",
    "project1 = r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\newTitle_Project.xlsx\"\n",
    "risk0 = r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\RiskFinal.xlsx\"\n",
    "stake1 = r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\New_StakeHolder_Abstract.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38244257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jqi22\\AppData\\Local\\Temp\\ipykernel_30792\\1831226226.py:1: DtypeWarning: Columns (3,7,8,12,13,18,27,28,29,30,31,38,39,40,41,42,43,44,45,50,51,53,54,59,63,64,65,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pj = pd.read_csv(project, sep = \",\")\n"
     ]
    }
   ],
   "source": [
    "pj = pd.read_csv(project, sep = \",\")\n",
    "risk1 = pd.read_excel(risk)\n",
    "stk = pd.read_csv(stake, sep = \",\")\n",
    "\n",
    "prj = pd.read_excel(project1)\n",
    "risk2 = pd.read_excel(risk0)\n",
    "stk1 = pd.read_excel(stake1)\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words\n",
    "\n",
    "# get the target text from original dataset to match\n",
    "nproject = pd.DataFrame(pj[\"Article Title\"])\n",
    "nrisk = pd.DataFrame(risk1[\"Abstract\"])\n",
    "nstack = pd.DataFrame(stk[\"Abstract\"])\n",
    "\n",
    "nrisk.Abstract = nrisk.Abstract.fillna(\"No Context\")\n",
    "\n",
    "stk1.name = stk1.name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "075eb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_dashes(droped: int)->pd.DataFrame:\n",
    "    dashes = pd.read_excel(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\adjustment.xlsx\")\n",
    "    dashes = dashes[dashes.frequency > droped].Words.to_list()\n",
    "    return set(dashes)\n",
    "\n",
    "stopwords |= reload_dashes(2)\n",
    "# filout = pd.read_excel(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\filtered.xlsx\")\n",
    "# stopwords |= set(filout.name.to_list())\n",
    "stopwords |= set([str(num) for num in range(1,100)])\n",
    "\n",
    "# manually add the words to the stopwords\n",
    "possiblew = {\"connections\", \"efficacy\", \"life\", \"This\"}\n",
    "stopwords |= possiblew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dda7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up the ORG whose frequency larger than 30\n",
    "fre = pd.read_csv(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\Examples\\ORG_frequency.csv\")\n",
    "chosenone = fre[fre.frequency >= 30]\n",
    "chosenone = chosenone[chosenone.name.apply(lambda x: x not in stopwords)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36eb1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(keyw: str, extracted: list):\n",
    "    extracted = list(set(extracted))\n",
    "    temp = []\n",
    "    for val in range(0, len(extracted)):\n",
    "        if re.sub(\"\\W\", \"\", extracted[val]) not in stopwords: temp.append(extracted[val])\n",
    "    del extracted\n",
    "    return [ele+\" \"+ keyw for ele in temp] if len(temp) else []\n",
    "\n",
    "def regex_match(keyword: str, args:str):\n",
    "    temp = args\n",
    "    lookbehind = rf\"(?<=\\b{keyword})(\\W\\W?\\w+)\"\n",
    "    lookforward = rf\"(\\w+\\W\\W?)(?=\\b{keyword}\\s)\"\n",
    "    return combination(keyword, re.findall(lookforward, temp)) + combination(keyword, re.findall(lookbehind, temp))\n",
    "\n",
    "def match_sentence(args:str, sent:str):\n",
    "    return re.findall(rf\"[^.]+{args}[^.]+\", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1755fe18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Thus, a dynamic risk control system is a valuable support for the successful completion of the sleeve grouting process',\n",
       " ' This study aims to develop an entropy-based sleeve grouting risk dynamic control system',\n",
       " ' Design/methodology/approach First, static risk assessment was conducted through the structured interview survey using the entropy weight method, followed by a dynamic risk control technique, where indicators were simulated through system dynamics containing causal loop diagrams and stock-and-flow diagrams',\n",
       " ' Findings Finally, three types of risk control models, namely, tortuous type, stable type and peak loop type, were developed in the entropy-based sleeve grouting risk dynamic control system and simulated using system dynamics in a real case',\n",
       " ' Originality/value Compared to traditional sleeve grouting risk management, the developed system enabled dynamic control over time']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args, sent=\"risk\",list(nrisk.iloc[0])\n",
    "match_sentence(args, sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d9aac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>university</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>european</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>water</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>department</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>faculty</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>chinese</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>councils</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>german</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>landscape</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  frequency\n",
       "0    university        514\n",
       "1      european        420\n",
       "2             &        358\n",
       "3         water        307\n",
       "4    department        305\n",
       "..          ...        ...\n",
       "214     faculty         30\n",
       "215     chinese         30\n",
       "216    councils         30\n",
       "217      german         30\n",
       "218   landscape         30\n",
       "\n",
       "[219 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-process of matching, but only apply for risk keyword abstraction\n",
    "def reduction(args: str, val: str):\n",
    "    args = str(args)\n",
    "    for item in args:\n",
    "        if item in [\"(\", \")\", \"+\"]: args = args.replace(item, \" \")\n",
    "    try:\n",
    "        return re.search(rf\"\\b{args}\\b\", val) != None\n",
    "    except Exception:\n",
    "        print(\"the currently word is: %s\", args, flags = re.IGNORECASE)\n",
    "    \n",
    "def dummy_project(args: pd.Series, val: str):\n",
    "    ags = str(args)\n",
    "    return val.find(args) != -1\n",
    "\n",
    "def match_attributes(args: str):\n",
    "    casualty=[]\n",
    "    \"\"\"args are the value from nrisk.Abstract\"\"\"\n",
    "#     res = prj[\"Article Title\"].apply(dummy_project, args = (args, ))\n",
    "#     casualty = prj[\"Article Title\"][res == True].to_list()\n",
    "#     res = risk2[\"Abstract\"].apply(reduction, args= (args, ))\n",
    "#     casualty = [*casualty, *risk2.Abstract[res==True].to_list()]\n",
    "    res = chosenone.name.apply(regex_match, args = (args,))\n",
    "    res = res[res.astype(bool)].explode()\n",
    "    # convert list into new rows (attention here the res is in pd.Series type not pd.DataFrame)\n",
    "#     res=stk2[\"stk\"].apply(reduction, args=(args,))\n",
    "#     res=stk2.stk[res==True]\n",
    "    casualty = [*casualty, *res.to_list()]\n",
    "#     print(casualty)\n",
    "    return casualty\n",
    "\n",
    "def write_json(new_data, filepath=jsfile):\n",
    "    with open(filepath,'r+') as file:\n",
    "          # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data.append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent = 4)\n",
    "\n",
    "# first run the first 500 lines and find out the frequency, delete the words whose frequency larger than 5\n",
    "def extraction(rnum: int = 500):\n",
    "    for value in nrisk.iloc[1:rnum].itertuples(): \n",
    "        if type(value.Abstract) is int or type(value.Abstract) is float: \n",
    "            print(value)\n",
    "            continue\n",
    "        write_json(match_attributes(value.Abstract))\n",
    "#         match_attributes(value.Abstract)\n",
    "    return\n",
    "# extraction(50)\n",
    "# nrisk.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"this is an example of how a spacy model can be used\"\n",
    "# texts = [\n",
    "#     \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "#     \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "# ]\n",
    "#\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "#     # Do something with the doc here\n",
    "#     print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# self-customerd function\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "@Language.component(\"info_integration\")\n",
    "def info_integration(doc):\n",
    "    ...\n",
    "\n",
    "\n",
    "def spacy_org(data):\n",
    "    vals = nlp(data)\n",
    "    # tags, ner = zip(*[(val.pos_, val.ents.label_) for val in vals if val.ents.label_ == \"ORG\"])\n",
    "    ner = [val.text for val in vals.ents if val.label_ == \"ORG\"]\n",
    "    return ner\n",
    "\n",
    "\n",
    "### LangChain ask\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "order = \" \".join(stk_sor.loc[0:15, :].Abstract)\n",
    "text = f\"please extract out organization within this word '{stk.Abstract[1]}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fre = pd.read_csv(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\Examples\\ORG_frequency.csv\")\n",
    "# chosenone = fre[fre.frequency >= 30]\n",
    "# chosenone = chosenone[chosenone.name.apply(lambda x: x not in stopwords)].reset_index(drop = True)\n",
    "def combination(keyw: str, extracted: list):\n",
    "    extracted = list(set(extracted))\n",
    "    temp = []\n",
    "    for val in range(0, len(extracted)):\n",
    "        if re.sub(\"\\W\", \"\", extracted[val]) not in stopwords: temp.append(extracted[val])\n",
    "    del extracted\n",
    "    return [ele + \" \" + keyw for ele in temp] if len(temp) else []\n",
    "\n",
    "\n",
    "def regex_match(keyword: str, args: str):\n",
    "    temp = args\n",
    "    lookbehind = rf\"(?<=\\b{keyword})(\\W\\W?\\w+)\"\n",
    "    lookforward = rf\"(\\w+\\W\\W?)(?=\\b{keyword}\\s)\"\n",
    "    return combination(keyword, re.findall(lookforward, temp)) + combination(keyword, re.findall(lookbehind, temp))\n",
    "\n",
    "\n",
    "# pre-process of matching, but only apply for risk keyword abstraction\n",
    "def reduction(args: str, val: str):\n",
    "    args = str(args)\n",
    "    for item in args:\n",
    "        if item in [\"(\", \")\", \"+\"]: args = args.replace(item, \" \")\n",
    "    try:\n",
    "        return re.search(rf\"\\b{args}\\b\", val) != None\n",
    "    except Exception:\n",
    "        print(\"the currently word is: %s\", args, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def dummy_project(args: pd.Series, val: str):\n",
    "    ags = str(args)\n",
    "    return val.find(args) != -1\n",
    "\n",
    "\n",
    "def match_attributes(args: str):\n",
    "    \"\"\"args are the value from nrisk.Abstract\"\"\"\n",
    "    res = prj_sor[\"Article Title\"].apply(dummy_project, args=(args,))\n",
    "    casualty = prj_sor[\"Article Title\"][res == True].to_list()\n",
    "    res = risk_sor[\"Abstract\"].apply(reduction, args=(args,))\n",
    "    casualty = [*casualty, *risk_sor.Abstract[res == True].to_list()]\n",
    "    # convert list into new rows (attention here the res is in pd.Series type not pd.DataFrame)\n",
    "    res = stk_sor[\"stk\"].apply(reduction, args=(args,))\n",
    "    res = stk_sor.stk[res == True]\n",
    "    casualty = [*casualty, *res.to_list()]\n",
    "    return casualty\n",
    "\n",
    "\n",
    "def write_json(new_data, filepath=jsfile):\n",
    "    with open(filepath, 'r+') as file:\n",
    "        # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data.append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent=4)\n",
    "\n",
    "\n",
    "# first run the first 500 lines and find out the frequency, delete the words whose frequency larger than 5\n",
    "def extraction(rnum: tuple = (0, 500)):\n",
    "    for value in nrisk.iloc[rnum[0]:rnum[1]].itertuples():\n",
    "        if type(value.Abstract) is int or type(value.Abstract) is float:\n",
    "            print(value)\n",
    "            continue\n",
    "        write_json(match_attributes(value.Abstract))\n",
    "    #         match_attributes(value.Abstract)\n",
    "    return\n",
    "\n",
    "\n",
    "interval: list = [(0, 50), (150, 400), (1000, 1500)]\n",
    "\n",
    "for items in interval:\n",
    "    extraction(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform not that good\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline\n",
    "#\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "#\n",
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data = tfds.load(\"conll2002\")\n",
    "ds_train, info, ds_test = data[\"train\"], data[\"dev\"], data[\"test\"]\n",
    "ds_train\n",
    "import tensorflow as tf\n",
    "\n",
    "physic_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "physic_devices\n",
    "# tf.config.experimental.set_memory_growth(physic_devices[0], True)\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d0d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk=pd.read_excel(r\"../ExcelData/Source/risk.xlsx\", index_col=None)\n",
    "# risk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b298d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125097\n"
     ]
    }
   ],
   "source": [
    "tar=risk.Abstract.to_list()\n",
    "print(len(tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b0956ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"risk_abstract.txt\", \"w\") as file:\n",
    "    for val in tar[:10_000]: file.write(f\"{val}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
