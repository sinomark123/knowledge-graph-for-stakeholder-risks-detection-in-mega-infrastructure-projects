{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install huggingface_hub\n",
    "! pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! huggingface-cli login\n",
    "# presnet setting has bugs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 182] 操作系统无法运行 %1。 Error loading \"d:\\Acer App\\Anaconda\\Anaconda3\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM \u001b[39mas\u001b[39;00m atcl\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[1;32md:\\Acer App\\Anaconda\\Anaconda3\\lib\\site-packages\\torch\\__init__.py:122\u001b[0m\n\u001b[0;32m    120\u001b[0m     err \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mWinError(last_error)\n\u001b[0;32m    121\u001b[0m     err\u001b[39m.\u001b[39mstrerror \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Error loading \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdll\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m or one of its dependencies.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    123\u001b[0m \u001b[39melif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     is_loaded \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 182] 操作系统无法运行 %1。 Error loading \"d:\\Acer App\\Anaconda\\Anaconda3\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM as atcl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_docs=[text.page_content for text in docs[:11]]\n",
    "query=\"promopt under construction.\"#risk_template(test_docs[0])\n",
    "inputs_ids=tokenizer(query, return_tensors=\"pt\").input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_result=model.generate(inputs_ids, max_length=1000)\n",
    "print(tokenizer.decode(generate_result[0], skip_special_tokens=True))\n",
    "\n",
    "# this is another embedding method, dont know whether comptable with current docs format\n",
    "# embed_tokenizer=AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embed_model=AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trans=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs={\"device\": 0}\n",
    "embedding=HuggingFaceEmbeddings(model_name=model_trans, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_llm=transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",# temperature=0,\n",
    "    top_p=0.15, top_k=15,\n",
    "    max_new_tokens=1060, repetition_penalty=1.2, do_sample=True\n",
    ")\n",
    "llm=HuggingFacePipeline(pipeline=prefix_llm, cache=False) # , model_kwargs={'temperature':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChiain Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "# Also pick keywords to label what ENTIRE sentence talked about from given tags: [environment science, environment studies, engineering civil, engineering industrial, management, engineering manufacturing, constructing building, constructing technology, transportation, finance]\n",
    "# keyword and tags\n",
    "instruction =\"\"\"help me to identify and summarize risks in the given sentence \"{words}\"? please mark risk out also list out the type.\n",
    "    If you cannot judge the organizations or there is no matched tag to label the entire sentence, please return 'None'. Show risks, type of risks appeared in sentence separately.\n",
    "\n",
    "    Answer example: \"\n",
    "    risks: what risk \\n\n",
    "\n",
    "    risk_type : \\n\n",
    "    Attention, you can only predict the output in above given criteria without any explanation and no more than 40 words.\n",
    "    \"\"\"\n",
    "system_prompt = \"You are an expert and summarization and expressing key ideas succintly\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"words\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "kwarky_list=[]\n",
    "testing_dict=[val.page_content for val in docs[:6]]\n",
    "text=docs[142].page_content\n",
    "output = llm_chain.run(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_generate(achain, context):\n",
    "    resp = await achain.acall(context)\n",
    "    print(resp)\n",
    "    return resp\n",
    "\n",
    "async def resp_feedback():\n",
    "  tasks = [async_generate(llm_chain, context) for context in testing_dict]\n",
    "  await asyncio.gather(*tasks)\n",
    "\n",
    "await resp_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load_qa_chain method to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=load_qa_chain(llm=llm, chain_type=\"stuff\")\n",
    "res_test=chain.run({\"words\": docs[4].page_content,\n",
    "                    \"question\": query,\n",
    "                    \"input_documents\": [docs[4]]})\n",
    "total_len=sum([len(docs[i].page_content) for i in range(len(docs))])\n",
    "print(res_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLMChain to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema={\n",
    "    \"properties\": {\n",
    "        \"risks\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"any unexpected event that can affect your project. e.g.,Construction cost overrun, Ship collision and grounding\"\n",
    "        },\n",
    "        \"risk_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"the area risk belong to. e.g. Financial, Technical \"\n",
    "        },\n",
    "        \"keywords\":{\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"tags\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"select most realted words from given list: [environment science, environment studies, engineering civil, engineering industrial, management, engineering manufacturing, constructing building, constructing technology, transportation, finance]\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"risks\", \"keywords\", \"tags\"]\n",
    "}\n",
    "sum_chain=create_extraction_chain(schema, llm, prompt=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_chain.run(\"this is an emergence project\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
