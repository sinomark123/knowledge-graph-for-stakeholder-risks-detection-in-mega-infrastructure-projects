{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3681ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpgrowth_py import fpgrowth\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "from spacy import displacy\n",
    "from flashtext import KeywordProcessor\n",
    "# import pyplot-express as px\n",
    "\n",
    "pathprefix = r\"D:\\Code Working Area\\Jupyter\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\"\n",
    "jsfile = pathprefix+\"\\\\Transactions.json\"\n",
    "\n",
    "project = pathprefix+\"\\\\Gg.csv\"\n",
    "risk = pathprefix+\"\\\\Risk_Simplified.xlsx\"\n",
    "stake = pathprefix+\"\\\\expansive.csv\"\n",
    "\n",
    "project1 = pathprefix+\"\\\\newTitle_Project.xlsx\"\n",
    "risk0 = pathprefix+\"\\\\RiskFinal.xlsx\"\n",
    "stake1 = pathprefix+\"\\\\New_StakeHolder_Abstract.xlsx\"\n",
    "\n",
    "stake2 = pathprefix+\"\\\\second_layer_stakeholder.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17bb1f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Code Working Area\\\\Jupyter\\\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\\\ExcelData\\\\second_layer_stakeholder.xlsx'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install regex\n",
    "# !pip install spacy\n",
    "# !pip install fpgrowth_py\n",
    "# !pip install pandas\n",
    "# !pip install seaborn\n",
    "# !pip install openpyxl\n",
    "# !python -m spacy install en_core_web_sm\n",
    "stake2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4136250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jqi22\\AppData\\Local\\Temp\\ipykernel_16576\\2096727506.py:1: DtypeWarning: Columns (3,7,8,12,13,18,27,28,29,30,31,38,39,40,41,42,43,44,45,50,51,53,54,59,63,64,65,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pj = pd.read_csv(project, sep = \",\")\n"
     ]
    }
   ],
   "source": [
    "pj = pd.read_csv(project, sep = \",\")\n",
    "risk1 = pd.read_excel(risk)\n",
    "stk = pd.read_csv(stake, sep = \",\")\n",
    "\n",
    "prj = pd.read_excel(project1)\n",
    "risk2 = pd.read_excel(risk0)\n",
    "stk1 = pd.read_excel(stake1)\n",
    "stk2 = pd.read_excel(stake2)\n",
    "stk2.dropna()\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6369d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target text from original dataset to match\n",
    "nproject = pd.DataFrame(pj[\"Article Title\"])\n",
    "nrisk = pd.DataFrame(risk1[\"Abstract\"])\n",
    "nstack = pd.DataFrame(stk[\"Abstract\"])\n",
    "\n",
    "nrisk.Abstract = nrisk.Abstract.fillna(\"No Context\")\n",
    "\n",
    "stk1.name = stk1.name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659980c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_dashes(droped: int)->pd.DataFrame:\n",
    "    dashes = pd.read_excel(pathprefix+\"\\\\adjustment.xlsx\")\n",
    "    dashes = dashes[dashes.frequency > droped].Words.to_list()\n",
    "    return set(dashes)\n",
    "\n",
    "stopwords |= reload_dashes(2)\n",
    "# filout = pd.read_excel(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\filtered.xlsx\")\n",
    "# stopwords |= set(filout.name.to_list())\n",
    "stopwords |= set([str(num) for num in range(1,100)])\n",
    "\n",
    "# manually add the words to the stopwords\n",
    "possiblew = {\"connections\", \"efficacy\", \"life\", \"This\"}\n",
    "stopwords |= possiblew"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# # split the stakeholder org words and count the frequency"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "efc75e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr1 = stk1.name.str.split(\" \").explode()\n",
    "# fre = pd.DataFrame(data=arr1.value_counts())\n",
    "# fre.reset_index(inplace = True)\n",
    "# fre = fre.rename(columns = {\"name\": \"frequency\", \"index\": \"name\"})\n",
    "# filtered = fre[fre.name.apply(lambda x: x not in stopwords)].reset_index(drop = True)\n",
    "# filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pick up the ORG whose frequency larger than 30"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bc8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fre = pd.read_csv(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\Examples\\ORG_frequency.csv\")\n",
    "# chosenone = fre[fre.frequency >= 30]\n",
    "# chosenone = chosenone[chosenone.name.apply(lambda x: x not in stopwords)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201b207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(keyw: str, extracted: list):\n",
    "    extracted = list(set(extracted))\n",
    "    temp = []\n",
    "    for val in range(0, len(extracted)):\n",
    "        if re.sub(\"\\W\", \"\", extracted[val]) not in stopwords: temp.append(extracted[val])\n",
    "    del extracted\n",
    "    return [ele+\" \"+ keyw for ele in temp] if len(temp) else []\n",
    "\n",
    "def regex_match(keyword: str, args:str):\n",
    "    temp = args\n",
    "    lookbehind = rf\"(?<=\\b{keyword})(\\W\\W?\\w+)\"\n",
    "    lookforward = rf\"(\\w+\\W\\W?)(?=\\b{keyword}\\s)\"\n",
    "    return combination(keyword, re.findall(lookforward, temp)) + combination(keyword, re.findall(lookbehind, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7de324e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre-process of matching, but only apply for risk keyword abstraction\n",
    "def reduction(args: str, val: str):\n",
    "    args = str(args)\n",
    "    for item in args:\n",
    "        if item in [\"(\", \")\", \"+\"]: args = args.replace(item, \" \")\n",
    "    try:\n",
    "        return re.search(rf\"\\b{args}\\b\", val) != None\n",
    "    except Exception:\n",
    "        print(\"the currently word is: %s\", args, flags = re.IGNORECASE)\n",
    "    \n",
    "def dummy_project(args: pd.Series, val: str):\n",
    "    ags = str(args)\n",
    "    return val.find(args) != -1\n",
    "\n",
    "def match_attributes(args: str):\n",
    "    \"\"\"args are the value from nrisk.Abstract\"\"\"\n",
    "    res = prj[\"Article Title\"].apply(dummy_project, args = (args, ))\n",
    "    casualty = prj[\"Article Title\"][res == True].to_list()\n",
    "    res = risk2[\"Abstract\"].apply(reduction, args= (args, ))\n",
    "    casualty = [*casualty, *risk2.Abstract[res==True].to_list()]\n",
    "    # convert list into new rows (attention here the res is in pd.Series type not pd.DataFrame)\n",
    "    res=stk2[\"stk\"].apply(reduction, args=(args,))\n",
    "    res=stk2.stk[res==True]\n",
    "    casualty = [*casualty, *res.to_list()]\n",
    "    return casualty\n",
    "\n",
    "def write_json(new_data, filepath=jsfile):\n",
    "    with open(filepath,'r+') as file:\n",
    "          # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data.append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent = 4)\n",
    "\n",
    "# first run the first 500 lines and find out the frequency, delete the words whose frequency larger than 5\n",
    "def extraction(rnum: tuple = (0, 500)):\n",
    "    for value in nrisk.iloc[rnum[0]:rnum[1]].itertuples(): \n",
    "        if type(value.Abstract) is int or type(value.Abstract) is float: \n",
    "            print(value)\n",
    "            continue\n",
    "        write_json(match_attributes(value.Abstract))\n",
    "#         match_attributes(value.Abstract)\n",
    "    return\n",
    "\n",
    "interval: list=[(0, 50), (150, 400), (1000, 1500)]\n",
    "\n",
    "for items in interval:\n",
    "    extraction(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de5bff4afa264249af5be0ae9373a7c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Acer App\\Anaconda\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jqi22\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b94006de65a4b9c85e4bad00f460f7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bef40f40c8348d8a0fb2997c1db824f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b62c31b87c624daba11b3d2b678810a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c8d64593bbf4d0d9ca7a69742b15af8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_14340\\3281376625.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"dslim/bert-base-NER\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoModelForTokenClassification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"dslim/bert-base-NER\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mnlp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"ner\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Acer App\\Anaconda\\Anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001B[0m in \u001B[0;36m__getattribute__\u001B[1;34m(cls, key)\u001B[0m\n\u001B[0;32m   1048\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"_\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;34m\"_from_config\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1049\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1050\u001B[1;33m         \u001B[0mrequires_backends\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_backends\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1051\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1052\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Acer App\\Anaconda\\Anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001B[0m in \u001B[0;36mrequires_backends\u001B[1;34m(obj, backends)\u001B[0m\n\u001B[0;32m   1027\u001B[0m     \u001B[1;31m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1028\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;34m\"torch\"\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mbackends\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"tf\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mbackends\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mis_torch_available\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mis_tf_available\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1029\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mPYTORCH_IMPORT_ERROR_WITH_TF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1030\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1031\u001B[0m     \u001B[1;31m# Raise the inverse error for PyTorch users trying to load TF classes\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fa9d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset element_spec={'ner': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'pos': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'tokens': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "data=tfds.load(\"conll2002\")\n",
    "ds_train, info, ds_test=data[\"train\"], data[\"dev\"], data[\"test\"]\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f073d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process on the stake keyword -- clean\n",
    "files: list = []\n",
    "with open(jsfile, \"r\") as file:\n",
    "    files = json.load(file)\n",
    "# files = pd.DataFrame({\"Words\": files})\n",
    "# files[\"expansion\"] = files.explode(\"Words\").reset_index(drop = True)\n",
    "\n",
    "# # sns.histplot(data = files, x = \"Words\", kde = True) # quiet useless\n",
    "# dashes = pd.DataFrame(files.value_counts()).reset_index(level = 0)\n",
    "# dashes.columns = [\"words\", \"frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a2902c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['uses risk', 'accident risk', 'operation risk', 'TAR'],\n",
       " ['safety risk'],\n",
       " ['early risk', 'aggregate risk'],\n",
       " ['different risk', 'Weibull'],\n",
       " ['construction project', 'struction project', 'political risk', 'CFA'],\n",
       " ['quantified risk',\n",
       "  'proposed risk',\n",
       "  'drought risk',\n",
       "  'annual risk',\n",
       "  'fed',\n",
       "  'CC',\n",
       "  'Correlation',\n",
       "  'Curve']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files=[x for x in files if x]\n",
    "files[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7d8c8eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# files.to_csv(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\Examples\\NewExample.csv\", index = False)\n",
    "# auxilary function, used to filter out stakeholder\n",
    "stk1 = pd.read_excel(stake1)\n",
    "def spacy_render(args: str):\n",
    "    doc = nlp(args)\n",
    "    displacy.render(doc, style = \"ent\")\n",
    "\n",
    "def quardoric(args: str):\n",
    "    doc=nlp(args)\n",
    "    return [tect.text for tect in doc.ents if tect.label_ == \"ORG\"]\n",
    "stk1.name = stk1.name.astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33010647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this step, we are going to handle on fp_growth\n",
    "# fpgrowth minSupRatio should not over than 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7642dd57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa01eb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e9ddac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "948f5d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5215caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05b0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e2a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
