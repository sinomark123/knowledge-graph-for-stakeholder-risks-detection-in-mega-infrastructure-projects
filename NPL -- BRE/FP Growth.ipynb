{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3681ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpgrowth_py import fpgrowth\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "from spacy import displacy\n",
    "from flashtext import KeywordProcessor\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "pathprefix = r\"D:\\Code Working Area\\Jupyter\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\"\n",
    "jsfile = pathprefix+\"\\\\Transactions.json\"\n",
    "\n",
    "project = pathprefix+\"\\\\Gg.csv\"\n",
    "risk = pathprefix+\"\\\\Risk_Simplified.xlsx\"\n",
    "stake = pathprefix+\"\\\\expansive.csv\"\n",
    "\n",
    "project1 = pathprefix+\"\\\\newTitle_Project.xlsx\"\n",
    "risk0 = pathprefix+\"\\\\RiskFinal.xlsx\"\n",
    "stake1 = pathprefix+\"\\\\New_StakeHolder_Abstract.xlsx\"\n",
    "\n",
    "stake2 = pathprefix+\"\\\\second_layer_stakeholder.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17bb1f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'D:\\\\Code Working Area\\\\Jupyter\\\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\\\ExcelData\\\\second_layer_stakeholder.xlsx'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install regex\n",
    "# !pip install spacy\n",
    "# !pip install fpgrowth_py\n",
    "# !pip install pandas\n",
    "# !pip install seaborn\n",
    "# !pip install openpyxl\n",
    "# !python -m spacy install en_core_web_sm\n",
    "stake2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4136250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jqi22\\AppData\\Local\\Temp\\ipykernel_25116\\2613482055.py:1: DtypeWarning: Columns (3,7,8,12,13,18,27,28,29,30,31,38,39,40,41,42,43,44,45,50,51,53,54,59,63,64,65,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pj = pd.read_csv(project, sep = \",\")\n"
     ]
    }
   ],
   "source": [
    "pj = pd.read_csv(project, sep = \",\")\n",
    "risk1 = pd.read_excel(risk)\n",
    "stk = pd.read_csv(stake, sep = \",\")\n",
    "\n",
    "prj = pd.read_excel(project1)\n",
    "risk2 = pd.read_excel(risk0)\n",
    "stk1 = pd.read_excel(stake1)\n",
    "stk2 = pd.read_excel(stake2)\n",
    "stk2.dropna()\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6369d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target text from original dataset to match\n",
    "nproject = pd.DataFrame(pj[\"Article Title\"])\n",
    "nrisk = pd.DataFrame(risk1[\"Abstract\"])\n",
    "nstack = pd.DataFrame(stk[\"Abstract\"])\n",
    "\n",
    "nrisk.Abstract = nrisk.Abstract.fillna(\"No Context\")\n",
    "\n",
    "stk1.name = stk1.name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659980c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_dashes(droped: int)->pd.DataFrame:\n",
    "    dashes = pd.read_excel(pathprefix+\"\\\\adjustment.xlsx\")\n",
    "    dashes = dashes[dashes.frequency > droped].Words.to_list()\n",
    "    return set(dashes)\n",
    "\n",
    "stopwords |= reload_dashes(2)\n",
    "# filout = pd.read_excel(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\filtered.xlsx\")\n",
    "# stopwords |= set(filout.name.to_list())\n",
    "stopwords |= set([str(num) for num in range(1,100)])\n",
    "\n",
    "# manually add the words to the stopwords\n",
    "possiblew = {\"connections\", \"efficacy\", \"life\", \"This\"}\n",
    "stopwords |= possiblew\n",
    "\n",
    "for word in stopwords:\n",
    "    en.vocab[word].is_stop=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec411d",
   "metadata": {},
   "source": [
    "# # split the stakeholder org words and count the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc75e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr1 = stk1.name.str.split(\" \").explode()\n",
    "# fre = pd.DataFrame(data=arr1.value_counts())\n",
    "# fre.reset_index(inplace = True)\n",
    "# fre = fre.rename(columns = {\"name\": \"frequency\", \"index\": \"name\"})\n",
    "# filtered = fre[fre.name.apply(lambda x: x not in stopwords)].reset_index(drop = True)\n",
    "# filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def clean_text(headline):\n",
    "    le=WordNetLemmatizer()\n",
    "    word_tokens=word_tokenize(headline)\n",
    "    tokens=[le.lemmatize(w) for w in word_tokens if w not in stopwords and len(w)>3]\n",
    "    cleaned_text=\" \".join(tokens)\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "stk.Abstract=stk.Abstract.str.lower()\n",
    "stk_lem=stk.Abstract.apply(clean_text).to_frame()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def spacy_org(data):\n",
    "    vals, partition=en(data),[]\n",
    "    for token in vals.ents:\n",
    "        if token.label_==\"ORG\": partition.append(token.text)\n",
    "    return partition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "0                      []\n1     [focus result bank]\n2                      []\n3                      []\n4                      []\n             ...         \n95                     []\n96                     []\n97                     []\n98                     []\n99                     []\nName: Abstract, Length: 100, dtype: object"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stk_lem.Abstract[0:100].apply(spacy_org)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0     []\n1     []\n2     []\n3     []\n4     []\n      ..\n95    []\n96    []\n97    []\n98    []\n99    []\nName: Abstract, Length: 100, dtype: object"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stk.Abstract[0:100].apply(spacy_org)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'spacy.tokens.span.Span'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[89], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m res\u001B[38;5;241m=\u001B[39men(stk\u001B[38;5;241m.\u001B[39mAbstract[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m res\u001B[38;5;241m.\u001B[39ments:\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(val, \u001B[43men\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtag_)\n",
      "File \u001B[1;32mD:\\Acer App\\Anaconda\\Anaconda3\\lib\\site-packages\\spacy\\language.py:1008\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[1;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[0;32m    987\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    988\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    989\u001B[0m     text: Union[\u001B[38;5;28mstr\u001B[39m, Doc],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    992\u001B[0m     component_cfg: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Dict[\u001B[38;5;28mstr\u001B[39m, Any]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    993\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Doc:\n\u001B[0;32m    994\u001B[0m     \u001B[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001B[39;00m\n\u001B[0;32m    995\u001B[0m \u001B[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001B[39;00m\n\u001B[0;32m    996\u001B[0m \u001B[38;5;124;03m    is preserved.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1006\u001B[0m \u001B[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001B[39;00m\n\u001B[0;32m   1007\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1008\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1009\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m component_cfg \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1010\u001B[0m         component_cfg \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32mD:\\Acer App\\Anaconda\\Anaconda3\\lib\\site-packages\\spacy\\language.py:1102\u001B[0m, in \u001B[0;36mLanguage._ensure_doc\u001B[1;34m(self, doc_like)\u001B[0m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m   1101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Doc(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab)\u001B[38;5;241m.\u001B[39mfrom_bytes(doc_like)\n\u001B[1;32m-> 1102\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE1041\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtype\u001B[39m(doc_like)))\n",
      "\u001B[1;31mValueError\u001B[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'spacy.tokens.span.Span'>"
     ]
    }
   ],
   "source": [
    "stk_org=stk.Abstract.apply(spacy_org)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "798bf1b3",
   "metadata": {},
   "source": [
    "## pick up the ORG whose frequency larger than 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bc8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fre = pd.read_csv(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\Examples\\ORG_frequency.csv\")\n",
    "# chosenone = fre[fre.frequency >= 30]\n",
    "# chosenone = chosenone[chosenone.name.apply(lambda x: x not in stopwords)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201b207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(keyw: str, extracted: list):\n",
    "    extracted = list(set(extracted))\n",
    "    temp = []\n",
    "    for val in range(0, len(extracted)):\n",
    "        if re.sub(\"\\W\", \"\", extracted[val]) not in stopwords: temp.append(extracted[val])\n",
    "    del extracted\n",
    "    return [ele+\" \"+ keyw for ele in temp] if len(temp) else []\n",
    "\n",
    "def regex_match(keyword: str, args:str):\n",
    "    temp = args\n",
    "    lookbehind = rf\"(?<=\\b{keyword})(\\W\\W?\\w+)\"\n",
    "    lookforward = rf\"(\\w+\\W\\W?)(?=\\b{keyword}\\s)\"\n",
    "    return combination(keyword, re.findall(lookforward, temp)) + combination(keyword, re.findall(lookbehind, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7de324e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre-process of matching, but only apply for risk keyword abstraction\n",
    "def reduction(args: str, val: str):\n",
    "    args = str(args)\n",
    "    for item in args:\n",
    "        if item in [\"(\", \")\", \"+\"]: args = args.replace(item, \" \")\n",
    "    try:\n",
    "        return re.search(rf\"\\b{args}\\b\", val) != None\n",
    "    except Exception:\n",
    "        print(\"the currently word is: %s\", args, flags = re.IGNORECASE)\n",
    "    \n",
    "def dummy_project(args: pd.Series, val: str):\n",
    "    ags = str(args)\n",
    "    return val.find(args) != -1\n",
    "\n",
    "def match_attributes(args: str):\n",
    "    \"\"\"args are the value from nrisk.Abstract\"\"\"\n",
    "    res = prj[\"Article Title\"].apply(dummy_project, args = (args, ))\n",
    "    casualty = prj[\"Article Title\"][res == True].to_list()\n",
    "    res = risk2[\"Abstract\"].apply(reduction, args= (args, ))\n",
    "    casualty = [*casualty, *risk2.Abstract[res==True].to_list()]\n",
    "    # convert list into new rows (attention here the res is in pd.Series type not pd.DataFrame)\n",
    "    res=stk2[\"stk\"].apply(reduction, args=(args,))\n",
    "    res=stk2.stk[res==True]\n",
    "    casualty = [*casualty, *res.to_list()]\n",
    "    return casualty\n",
    "\n",
    "def write_json(new_data, filepath=jsfile):\n",
    "    with open(filepath,'r+') as file:\n",
    "          # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data.append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent = 4)\n",
    "\n",
    "# first run the first 500 lines and find out the frequency, delete the words whose frequency larger than 5\n",
    "def extraction(rnum: tuple = (0, 500)):\n",
    "    for value in nrisk.iloc[rnum[0]:rnum[1]].itertuples(): \n",
    "        if type(value.Abstract) is int or type(value.Abstract) is float: \n",
    "            print(value)\n",
    "            continue\n",
    "        write_json(match_attributes(value.Abstract))\n",
    "#         match_attributes(value.Abstract)\n",
    "    return\n",
    "\n",
    "interval: list=[(0, 50), (150, 400), (1000, 1500)]\n",
    "\n",
    "for items in interval:\n",
    "    extraction(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7bc8c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Stakeholder salience has proven an elusive mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Stakeholder analysis is nowadays considered as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Corporate stakeholder culture is a new study f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>According to research, stakeholder disappointm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Understanding stakeholder dynamics and their i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48336</th>\n",
       "      <td>48336</td>\n",
       "      <td>Purpose - The purpose of this paper is to expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48337</th>\n",
       "      <td>48337</td>\n",
       "      <td>Globally, banks are entering a new era. Settin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48338</th>\n",
       "      <td>48338</td>\n",
       "      <td>There has been an increased interest in utiliz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48339</th>\n",
       "      <td>48339</td>\n",
       "      <td>The corporation has been the object of scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48340</th>\n",
       "      <td>48340</td>\n",
       "      <td>Some ecological economists have advocated part...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           Abstract\n",
       "0               0  Stakeholder salience has proven an elusive mea...\n",
       "1               1  Stakeholder analysis is nowadays considered as...\n",
       "2               2  Corporate stakeholder culture is a new study f...\n",
       "3               3  According to research, stakeholder disappointm...\n",
       "4               4  Understanding stakeholder dynamics and their i...\n",
       "...           ...                                                ...\n",
       "48336       48336  Purpose - The purpose of this paper is to expl...\n",
       "48337       48337  Globally, banks are entering a new era. Settin...\n",
       "48338       48338  There has been an increased interest in utiliz...\n",
       "48339       48339  The corporation has been the object of scienti...\n",
       "48340       48340  Some ecological economists have advocated part...\n",
       "\n",
       "[48341 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "385442a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c8b2d83fb04942a87492adbadc8db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Acer App\\Anaconda\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jqi22\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "# Perform not that good\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline\n",
    "#\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "#\n",
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "stk.loc[:, \"Abstract\"]=stk.Abstract.str.lower()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "\"stakeholder analysis is nowadays considered as an important part of management of all institutions. for all organisations it is necessary to analysed in detail all individuals or groups that can affect or are affected by the organisations' activities. this article deals with the detailed stakeholder analysis and identification in the bank sector. in the first part of this article there are generally defined stakeholders and the basis of stakeholder approach. the second part focuses on results of the author's pilot research of stakeholders in bank sector. the analysis of stakeholders is carried out on the basis of stakeholder circle methodology. the emphasis is on the identification and prioritisation of key stakeholders of bank institutions. the main objective of this article is verification of application possibilities of the stakeholder analysis and the stakeholder circle methodology in the bank sector and detail identification and prioritisation of key stakeholder groups of bank institutions.\""
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stk.Abstract[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fa9d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'ner': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'pos': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'tokens': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "data=tfds.load(\"conll2002\")\n",
    "ds_train, info, ds_test=data[\"train\"], data[\"dev\"], data[\"test\"]\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715677ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physic_devices=tf.config.list_physical_devices(\"GPU\")\n",
    "physic_devices\n",
    "# tf.config.experimental.set_memory_growth(physic_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1cac1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f073d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process on the stake keyword -- clean\n",
    "files: list = []\n",
    "with open(jsfile, \"r\") as file:\n",
    "    files = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a2902c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['uses risk', 'accident risk', 'operation risk', 'TAR'],\n",
       " ['safety risk'],\n",
       " ['early risk', 'aggregate risk'],\n",
       " ['different risk', 'Weibull'],\n",
       " ['construction project', 'struction project', 'political risk', 'CFA'],\n",
       " ['quantified risk',\n",
       "  'proposed risk',\n",
       "  'drought risk',\n",
       "  'annual risk',\n",
       "  'fed',\n",
       "  'CC',\n",
       "  'Correlation',\n",
       "  'Curve']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7d8c8eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# files.to_csv(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\Examples\\NewExample.csv\", index = False)\n",
    "# auxilary function, used to filter out stakeholder\n",
    "stk1 = pd.read_excel(stake1)\n",
    "def spacy_render(args: str):\n",
    "    doc = nlp(args)\n",
    "    displacy.render(doc, style = \"ent\")\n",
    "\n",
    "def quardoric(args: str):\n",
    "    doc=nlp(args)\n",
    "    return [tect.text for tect in doc.ents if tect.label_ == \"ORG\"]\n",
    "stk1.name = stk1.name.astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33010647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this step, we are going to handle on fp_growth\n",
    "# fpgrowth minSupRatio should not over than 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7642dd57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa01eb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e9ddac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "948f5d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5215caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05b0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e2a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
