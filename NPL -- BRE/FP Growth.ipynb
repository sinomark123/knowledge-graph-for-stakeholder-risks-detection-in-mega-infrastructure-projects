{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3681ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas\n",
    "from fpgrowth_py import fpgrowth\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "from spacy import displacy\n",
    "from flashtext import KeywordProcessor\n",
    "import ahocorasick\n",
    "import nltk\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from enum import Enum\n",
    "\n",
    "pathprefix = r\"D:\\Code Working Area\\Jupyter\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\"\n",
    "jsfile = pathprefix+\"\\\\Transactions.json\"\n",
    "stop_words=pathprefix+\"\\\\stop_words\\\\stop_words_stakeholder.txt\"\n",
    "unknow_long_phrase=pathprefix+\"\\\\stop_words\\\\unknow_long.txt\"\n",
    "\n",
    "project_sor = pathprefix+\"\\\\Source\\\\project.csv\"\n",
    "risk_sor = pathprefix+\"\\\\Source\\\\risk.xlsx\"\n",
    "stake_sor = pathprefix+\"\\\\Source\\\\stakeholder.csv\"\n",
    "\n",
    "project_key = pathprefix+\"\\\\project_keyword\\\\Project_keyword.xlsx\"\n",
    "risk_key_path = pathprefix+\"\\\\risk_keyword\\\\Fourth_edition.xlsx\"\n",
    "stake_key = pathprefix+\"\\\\stakeholder_keyword\\\\third_layer_iteration_one_stakeholder.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4136250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jqi22\\AppData\\Local\\Temp\\ipykernel_25796\\3169248101.py:1: DtypeWarning: Columns (3,7,8,12,13,18,27,28,29,30,31,38,39,40,41,42,43,44,45,50,51,53,54,59,63,64,65,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  prj_sor = pd.read_csv(project_sor, sep = \",\")\n"
     ]
    }
   ],
   "source": [
    "prj_sor = pd.read_csv(project_sor, sep = \",\")\n",
    "risk_sor = pd.read_excel(risk_sor)\n",
    "stk_sor = pd.read_csv(stake_sor, sep = \",\")\n",
    "\n",
    "prj_key = pd.read_excel(project_key, index_col=None)\n",
    "risk_key = pd.read_excel(risk_key_path, index_col=None).dropna(inplace=True, how = \"any\")\n",
    "stk_key = pd.read_excel(stake_key, index_col=None)\n",
    "stk_key.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6369d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target text from original dataset to match\n",
    "project = pd.DataFrame(prj_sor[\"Article Title\"])\n",
    "risk = pd.DataFrame(risk_sor[\"Abstract\"])\n",
    "stake = pd.DataFrame(stk_sor[\"Abstract\"])\n",
    "\n",
    "risk.dropna(inplace=True, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'Abstract'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m stk_key\u001B[38;5;241m.\u001B[39mAbstract \u001B[38;5;241m=\u001B[39m stk_key\u001B[38;5;241m.\u001B[39mAbstract\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mlower()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Do I really need below two steps?\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m risk_key\u001B[38;5;241m.\u001B[39mAbstract \u001B[38;5;241m=\u001B[39m \u001B[43mrisk_key\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAbstract\u001B[49m\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mstr\u001B[39m)\n\u001B[0;32m      5\u001B[0m stk_key\u001B[38;5;241m.\u001B[39mAbstract \u001B[38;5;241m=\u001B[39m stk_key\u001B[38;5;241m.\u001B[39mAbstract\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mstr\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'Abstract'"
     ]
    }
   ],
   "source": [
    "stk_key.Abstract = stk_key.Abstract.str.lower()\n",
    "\n",
    "# Do I really need below two steps?\n",
    "risk_key.Abstract = risk_key.Abstract.astype(str)\n",
    "stk_key.Abstract = stk_key.Abstract.astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# do this for keyword abstract\n",
    "prjkey_list = prj_key[\"Article\"].to_list()\n",
    "riskey_list = risk_key[\"Abstract\"].to_list()\n",
    "stkey_list = stk_key.Abstract.to_list()\n",
    "### initialize keywordprocessor class\n",
    "keypro = KeywordProcessor()\n",
    "keypro.add_keywords_from_list(prjkey_list + riskey_list + stkey_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # disable=[\"attribute_ruler\"]\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "nlp.pipe_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659980c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_dashes(droped: int)->pd.DataFrame:\n",
    "    dashes = pd.read_excel(pathprefix+\"\\\\stop_words\\\\adjustment.xlsx\")\n",
    "    dashes = dashes[dashes.frequency > droped].Words.to_list()\n",
    "    return set(dashes)\n",
    "\n",
    "stopwords |= reload_dashes(2)\n",
    "# filout = pd.read_excel(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\filtered.xlsx\")\n",
    "# stopwords |= set(filout.name.to_list())\n",
    "stopwords |= set([str(num) for num in range(1,100)])\n",
    "\n",
    "# manually add the words to the stopwords\n",
    "possiblew = {\"connections\", \"efficacy\", \"life\", \"This\"}\n",
    "stopwords |= possiblew\n",
    "\n",
    "for word in stopwords:\n",
    "    nlp.vocab[word].is_stop=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec411d",
   "metadata": {},
   "source": [
    "# # split the stakeholder org words and count the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "def clean_text(headline):\n",
    "    le=WordNetLemmatizer()\n",
    "    word_tokens=word_tokenize(headline)\n",
    "    tokens=[le.lemmatize(w) for w in word_tokens if w not in stopwords and len(w)>3]\n",
    "    cleaned_text=\" \".join(tokens)\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "stk_sor.Abstract=stk_sor.Abstract.str.lower()\n",
    "# stk_lem=stk.Abstract.apply(clean_text).to_frame()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "'this is an example of how a spacy model can be used'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is an example of how a spacy model can be used\"\n",
    "# texts = [\n",
    "#     \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "#     \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "# ]\n",
    "#\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "#     # Do something with the doc here\n",
    "#     print([(ent.text, ent.label_) for ent in doc.ents])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# self-customerd function\n",
    "from spacy.language import Language\n",
    "@Language.component(\"info_integration\")\n",
    "def info_integration(doc):\n",
    "    ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def spacy_org(data):\n",
    "    vals=nlp(data)\n",
    "    # tags, ner = zip(*[(val.pos_, val.ents.label_) for val in vals if val.ents.label_ == \"ORG\"])\n",
    "    ner=[val.text for val in vals.ents if val.label_==\"ORG\"]\n",
    "    return ner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LangChain ask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm=OpenAI(temperature=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "order=\" \".join(stk_sor.loc[0:15,:].Abstract)\n",
    "text=f\"please extract out organization within this word '{stk.Abstract[1]}'\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "798bf1b3",
   "metadata": {},
   "source": [
    "## pick up the ORG whose frequency larger than 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bc8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fre = pd.read_csv(r\"D:\\Code Working Area\\Python\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\\Examples\\ORG_frequency.csv\")\n",
    "# chosenone = fre[fre.frequency >= 30]\n",
    "# chosenone = chosenone[chosenone.name.apply(lambda x: x not in stopwords)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201b207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(keyw: str, extracted: list):\n",
    "    extracted = list(set(extracted))\n",
    "    temp = []\n",
    "    for val in range(0, len(extracted)):\n",
    "        if re.sub(\"\\W\", \"\", extracted[val]) not in stopwords: temp.append(extracted[val])\n",
    "    del extracted\n",
    "    return [ele+\" \"+ keyw for ele in temp] if len(temp) else []\n",
    "\n",
    "def regex_match(keyword: str, args:str):\n",
    "    temp = args\n",
    "    lookbehind = rf\"(?<=\\b{keyword})(\\W\\W?\\w+)\"\n",
    "    lookforward = rf\"(\\w+\\W\\W?)(?=\\b{keyword}\\s)\"\n",
    "    return combination(keyword, re.findall(lookforward, temp)) + combination(keyword, re.findall(lookbehind, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7de324e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre-process of matching, but only apply for risk keyword abstraction\n",
    "def reduction(args: str, val: str):\n",
    "    args = str(args)\n",
    "    for item in args:\n",
    "        if item in [\"(\", \")\", \"+\"]: args = args.replace(item, \" \")\n",
    "    try:\n",
    "        return re.search(rf\"\\b{args}\\b\", val) != None\n",
    "    except Exception:\n",
    "        print(\"the currently word is: %s\", args, flags = re.IGNORECASE)\n",
    "    \n",
    "def dummy_project(args: pd.Series, val: str):\n",
    "    ags = str(args)\n",
    "    return val.find(args) != -1\n",
    "\n",
    "def match_attributes(args: str):\n",
    "    \"\"\"args are the value from nrisk.Abstract\"\"\"\n",
    "    res = prj_sor[\"Article Title\"].apply(dummy_project, args = (args, ))\n",
    "    casualty = prj_sor[\"Article Title\"][res == True].to_list()\n",
    "    res = risk_sor[\"Abstract\"].apply(reduction, args= (args, ))\n",
    "    casualty = [*casualty, *risk_sor.Abstract[res==True].to_list()]\n",
    "    # convert list into new rows (attention here the res is in pd.Series type not pd.DataFrame)\n",
    "    res=stk_sor[\"stk\"].apply(reduction, args=(args,))\n",
    "    res=stk_sor.stk[res==True]\n",
    "    casualty = [*casualty, *res.to_list()]\n",
    "    return casualty\n",
    "\n",
    "def write_json(new_data, filepath=jsfile):\n",
    "    with open(filepath,'r+') as file:\n",
    "          # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data.append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent = 4)\n",
    "\n",
    "# first run the first 500 lines and find out the frequency, delete the words whose frequency larger than 5\n",
    "def extraction(rnum: tuple = (0, 500)):\n",
    "    for value in nrisk.iloc[rnum[0]:rnum[1]].itertuples(): \n",
    "        if type(value.Abstract) is int or type(value.Abstract) is float: \n",
    "            print(value)\n",
    "            continue\n",
    "        write_json(match_attributes(value.Abstract))\n",
    "#         match_attributes(value.Abstract)\n",
    "    return\n",
    "\n",
    "interval: list=[(0, 50), (150, 400), (1000, 1500)]\n",
    "\n",
    "for items in interval:\n",
    "    extraction(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385442a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform not that good\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline\n",
    "#\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "#\n",
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fa9d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'ner': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'pos': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'tokens': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "data=tfds.load(\"conll2002\")\n",
    "ds_train, info, ds_test=data[\"train\"], data[\"dev\"], data[\"test\"]\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715677ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physic_devices=tf.config.list_physical_devices(\"GPU\")\n",
    "physic_devices\n",
    "# tf.config.experimental.set_memory_growth(physic_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1cac1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### extract keyword from Abstract"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48a2902c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d8c8eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33010647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7642dd57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa01eb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e9ddac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "948f5d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5215caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05b0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e2a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
