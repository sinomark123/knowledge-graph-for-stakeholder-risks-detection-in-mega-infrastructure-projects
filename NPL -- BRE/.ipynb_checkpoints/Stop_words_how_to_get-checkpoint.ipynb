{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from flashtext import KeywordProcessor\n",
    "import nltk\n",
    "\n",
    "pathprefix = r\"D:\\Code Working Area\\Jupyter\\knowledge-graph-for-stakeholder-risks-detection-in-mega-infrastructure-projects\\ExcelData\"\n",
    "jsfile = pathprefix+\"\\\\Transactions.json\"\n",
    "\n",
    "project_sor = pathprefix+\"\\\\Source\\\\project.csv\"\n",
    "risk_sor = pathprefix+\"\\\\Source\\\\risk.xlsx\"\n",
    "stake_sor = pathprefix+\"\\\\Source\\\\stakeholder.csv\"\n",
    "\n",
    "project_key = pathprefix+\"\\\\project_keyword\\\\Project_keyword.xlsx\"\n",
    "risk_key = pathprefix+\"\\\\risk_keyword\\\\Risk_keyword.xlsx\"\n",
    "stake_key = pathprefix+\"\\\\stakeholder_keyword\\\\third_layer_iteration_one_stakeholder.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_sor = pd.read_csv(project_sor, sep = \",\")\n",
    "risk_sor = pd.read_excel(risk_sor)\n",
    "stk_sor = pd.read_csv(stake_sor, sep = \",\")\n",
    "\n",
    "prj_key = pd.read_excel(project_key, index_col=None)\n",
    "risk_key = pd.read_excel(risk_key, index_col=None)\n",
    "stk_key = pd.read_excel(stake_key, index_col=None)\n",
    "stk_key.dropna(inplace=True)\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target text from original dataset to match\n",
    "project = pd.DataFrame(prj_sor[\"Article Title\"])\n",
    "risk = pd.DataFrame(risk_sor[\"Abstract\"])\n",
    "stake = pd.DataFrame(stk_sor[\"Abstract\"])\n",
    "\n",
    "risk.dropna(inplace=True, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk_key.name = stk_key.name.str.lower()\n",
    "\n",
    "# Do I really need below two steps?\n",
    "risk_key.Abstract=risk_key.Abstract.astype(str)\n",
    "stk_key.Abstract=stk_key.Abstract.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this for keyword abstract\n",
    "prjkey_list = prj_key[\"Article\"].to_list()\n",
    "riskey_list = risk_key[\"Abstract\"].to_list()\n",
    "stkey_list = stk_key.Abstract.to_list()\n",
    "### initialize keywordprocessor class\n",
    "keypro = KeywordProcessor()\n",
    "keypro.add_keywords_from_list(prjkey_list + riskey_list + stkey_list)\n",
    "\n",
    "\n",
    "def abst(args: str):\n",
    "    return keypro.extract_keywords(args)\n",
    "\n",
    "### extract out the keyword only in nrisk\n",
    "res = risk.Abstract.apply(abst)\n",
    "temp = res.explode(ignore_index=True).dropna(how=\"any\")\n",
    "tempres = temp.value_counts().to_frame()\n",
    "tempres[\"words\"] = tempres.index\n",
    "tempres.reset_index(drop=True, inplace=True)\n",
    "tempres.words = tempres.words.str.lower()\n",
    "\n",
    "# use NLTK tag on words, then only get Noun-realted words\n",
    "# store the rest corpous as stop_words, named NLTK_reuslt.xlsx\n",
    "tempres[\"word type\"] = tempres.words.apply(lambda row: nltk.pos_tag(nltk.word_tokenize(row))[0][1])\n",
    "nnres = tempres[tempres[\"word type\"].apply(lambda x: x in [\"NN\", \"NNP\", \"NNS\", \"NNPS\"])]\n",
    "remain_res = tempres.drop(nnres.index)\n",
    "\n",
    "# kick out project and risk stock phrase within dataset\n",
    "def risk_and_project(data):\n",
    "    if \"project\" in data or \"risk\" in data: return False\n",
    "    return True\n",
    "\n",
    "# by counting the frequency, extract first 2K words and combine together\n",
    "filter_stk = nnres[nnres.words.apply(risk_and_project)]\n",
    "require = \" \".join(filter_stk.words[0:1900].to_list())\n",
    "\n",
    "# use LangChain to identify meanless words within sentence\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "text = f\"please split sentence as single word and filter out any word that cannot be an organization or stakeholder, return me a list of combination, given the text '{require}'\"\n",
    "filter_out = llm(text)\n",
    "# writer=pd.ExcelWriter(\"NLTK_result.xlsx\")\n",
    "# for name, val in remain_res.groupby(\"word type\"):\n",
    "#     val.to_excel(writer, sheet_name=name, index=False)\n",
    "filter_out = [val.lower() for val in filter_out.strip(\"\\n\").split(\", \")]\n",
    "\n",
    "# now we get the meanless stakeholder word, which stored in \"filter_out\", we split them from dataset\n",
    "mask = filter_stk.words.str.contains(\"|\".join(filter_out))\n",
    "stk_filter = filter_stk[~mask]\n",
    "filter_out = [*filter_out, *stk_filter.words[0:78].to_list()]\n",
    "stk2_mask = stk_key.Abstract.str.lower().str.contains(\"|\".join(filter_out))\n",
    "stk2_remain = stk_key[~stk2_mask]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
