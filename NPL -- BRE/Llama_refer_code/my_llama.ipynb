{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIbKb5YxemyS"
      },
      "outputs": [],
      "source": [
        "# !hugging_hub login\n",
        "# !export HUGGINFACE_KEY_API=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY5wciu3ql_-",
        "outputId": "a0bf92d7-e443-4c19-d7c9-de11b883860d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DISmp5XgLFmp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "from typing import List\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def get_prompt_by_list(instruction: List[str], new_system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
        "  SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "  return [B_SYS+SYSTEM_PROMPT+instr+E_INST for instr in instruction]\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buMgvk31qnoT"
      },
      "outputs": [],
      "source": [
        "# !pip install virtualenv\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install langchain\n",
        "!pip install sentence_transformers\n",
        "!pip install bitsandbytes-cuda110\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "# !pip3 install torch torchvision torchaudio\n",
        "# !pip install packaging\n",
        "# !pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iukH-HFNqrgR"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, BitsAndBytesConfig, TextStreamer, LlamaForCausalLM\n",
        "import torch\n",
        "import sys\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate, StringPromptTemplate\n",
        "from langchain.chains import LLMChain, create_extraction_chain\n",
        "# sys.path.append(\"/content/drive/MyDrive/CoLab/Llama2/bin/python3.10/site-packages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VxjJsqAZqZ4"
      },
      "outputs": [],
      "source": [
        "# very important part, we quantilize Llama model by these codes accerlate\n",
        "quant_config=BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    # load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KKoFfluuFgv"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi\n",
        "# !sudo fuser -v /dev/nvidia0\n",
        "# !sudo kill -9 406"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8612Mrgnqsnl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3ff54cfc4bda47a3ab852803342ca74a",
            "5c7267210abf456f949cf5fb078ccc4f",
            "818d3f68cf774c629aceecf6cf5fc10d",
            "662f486b6886486e9f240858d741182f",
            "a761d0e2b31d44f9b1cdd39c25f5d997",
            "048570db65514aa0af742234b4b9c5d1",
            "36c2d807e77f4798902967b58b1896b5",
            "5303253263c24279bb26cdcc2cec9640",
            "1b0d8e1d622a4ac49e7d26f4297c7029",
            "325125957e2d4810adc568c7531c2707",
            "a5be838ac2fe4e39a2d853947999cf29"
          ]
        },
        "outputId": "95703e4e-396d-4ecc-99b9-c8e8d415b45d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ff54cfc4bda47a3ab852803342ca74a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers.pipelines.base import AutoConfig\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/CoLab/LLama7b\")  # daryl149/llama-2-7b-chat-hf\n",
        "model=LlamaForCausalLM.from_pretrained(\"/content/drive/MyDrive/CoLab/LLama7b\",quantization_config=quant_config,device_map=\"auto\",)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# model=AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/CoLab/LLama7b\", trust_remote_code=False, torch_dtype=torch.float16)\n",
        "# config=AutoConfig.from_pretrained(\n",
        "#     pretrained_model_name_or_path=\"/content/drive/MyDrive/CoLab/LLama7b\",\n",
        "#     top_p=0.67, top_k=15, repetition_penalty=1.2\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZThinOnRtWb"
      },
      "outputs": [],
      "source": [
        "# load on text splitter and initialize text loading\n",
        "loader=TextLoader(r\"/content/drive/MyDrive/CoLab/Test_data/risk_abstract.txt\")\n",
        "text=loader.load()\n",
        "text_splitter=CharacterTextSplitter(chunk_size=4050, chunk_overlap=20)\n",
        "docs=text_splitter.split_documents(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9960T4FNCQac"
      },
      "source": [
        "Do we need a Promopt template? Yes we need\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64jelDr8sajf",
        "outputId": "501af22c-7481-45be-ea84-4c2f859862b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 23 15:19:00 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0    33W /  70W |   5015MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "device=f\"cuda: {cuda.current_device()}\"\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRUwGUexgrHy"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "# Also pick keywords to label what ENTIRE sentence talked about from given tags: [environment science, environment studies, engineering civil, engineering industrial, management, engineering manufacturing, constructing building, constructing technology, transportation, finance]\n",
        "# keyword and tags\n",
        "def consutrct_instruction(words: str):\n",
        "  instruction = f\"\"\"Help me to identify and summarize risks may raised in the given sentence \"{words}\".\n",
        "    Please return risk may caused in the sentence and also pick risk type from given list:\n",
        "    [environment science, environment studies, engineering civil, engineering industrial, management, engineering manufacturing, constructing building, constructing technology, transportation, finance]\n",
        "    If you cannot judge the organizations or there is no matched tag to label the entire sentence, please return 'None'.\n",
        "\n",
        "    Answer example:\n",
        "    risks:  \\n\n",
        "\n",
        "    risk_type: \\n\n",
        "\n",
        "    Attention, each risk should be summarized in 4 words and summarize with suffix word 'risk'\n",
        "    Attention, you can only output risk type with words from above list.\n",
        "    Attention, only risks and risk_type should be presented.\n",
        "    Attention, you can only predict the output in the above given criteria.\n",
        "    \"\"\"\n",
        "  return instruction\n",
        "system_prompt = \"You are an expert and summarization and expressing key ideas succintly\"\n",
        "\n",
        "prefix_llm=transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True, batch_size=4,\n",
        "    task=\"text-generation\",# temperature=0,\n",
        "    top_p=0.15, top_k=15,\n",
        "    max_new_tokens=1060, repetition_penalty=1.2, do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7NnN5ADrurP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "a paradigm for direct huggingface transformer loading usage\n",
        "what about agenerate?\n",
        "\"\"\"\n",
        "async def generate(text, modelv):\n",
        "    prompt,number = get_prompt(consutrct_instruction(text)),text[:6]\n",
        "    with torch.autocast('cuda', dtype=torch.float16):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "        print(len(inputs))\n",
        "        outputs = modelv.generate(**inputs,\n",
        "                                 max_new_tokens=256,\n",
        "                                 eos_token_id=tokenizer.eos_token_id,\n",
        "                                 pad_token_id=tokenizer.eos_token_id,\n",
        "                                top_p=0.15, top_k=15, repetition_penalty=1.2,do_sample=True\n",
        "                                 )\n",
        "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
        "        final_outputs = remove_substring(final_outputs, prompt)\n",
        "\n",
        "    return final_outputs+\"  \"+str(number), number # , outputs\n",
        "\n",
        "def generate_noasync(text, modelv):\n",
        "    prompt = get_prompt(consutrct_instruction(text))\n",
        "    with torch.autocast('cuda', dtype=torch.float16):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "        print(len(inputs))\n",
        "        outputs = modelv.generate(**inputs,\n",
        "                                 max_new_tokens=256,\n",
        "                                 eos_token_id=tokenizer.eos_token_id,\n",
        "                                 pad_token_id=tokenizer.eos_token_id,\n",
        "                                top_p=0.15, top_k=15, repetition_penalty=1.2,do_sample=True\n",
        "                                 )\n",
        "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
        "        final_outputs = remove_substring(final_outputs, prompt)\n",
        "\n",
        "    return final_outputs # , outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvYYpcP_NEsK"
      },
      "outputs": [],
      "source": [
        "words=docs[0].page_content\n",
        "context_pure=[get_prompt(consutrct_instruction(docs[i].page_content), system_prompt) for i in range(6)]\n",
        "template = get_prompt(consutrct_instruction(words), system_prompt)\n",
        "# print(template)\n",
        "# res=generate(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaKML0OoWPGE"
      },
      "outputs": [],
      "source": [
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncq3a2L0RU2_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "796ac8eb-69ec-4415-f65d-42c7fbd14435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnicodeEncodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-41f3319f8678>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Run the async function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/CoLab/Llama2/output2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-41f3319f8678>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/CoLab/Llama2/output2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m           \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\\n\\n\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moutput_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 482-483: ordinal not in range(128)"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# # Measure execution time of generate\n",
        "# sync_start_time = time.time()\n",
        "# generate_noasync(text, prefix_llm)\n",
        "# sync_end_time = time.time()\n",
        "# sync_execution_time = sync_end_time - sync_start_time\n",
        "# print(\"Async generate execution time:\", async_execution_time)\n",
        "excutorpool=ThreadPoolExecutor(1)\n",
        "\n",
        "async def test_generate(text):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    start_time = time.time()\n",
        "    await generate(text, prefix_llm)\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    return execution_time\n",
        "\n",
        "async def main():\n",
        "    start, lenth, interval=1154, 2000, 5\n",
        "    text = [f\"{val}\"+docs[val].page_content for val in range(start, lenth)]\n",
        "    output_list: List[str]=[]\n",
        "    task, mints, maxts=[], 85, 10\n",
        "\n",
        "    # Measure execution time of async generate\n",
        "    async_start_time = time.time()\n",
        "    for num in range(0, lenth, interval):\n",
        "      round_time=time.time()\n",
        "      for txt in text[num: num+interval]: task.append(asyncio.create_task(generate(txt, model)))\n",
        "      output_list.extend(await asyncio.gather(*task, return_exceptions=True))\n",
        "      task.clear()\n",
        "      round_end=time.time()-round_time\n",
        "      if round_end>maxts: maxts=round_end\n",
        "      elif round_end<mints: mints=round_end\n",
        "\n",
        "      if len(output_list)>50:\n",
        "        with open(\"/content/drive/MyDrive/CoLab/Llama2/output2.txt\", \"a\") as file:\n",
        "          file.writelines(val[0]+\"\\n\\n\\n\" for val in output_list)\n",
        "        output_list.clear()\n",
        "\n",
        "    async_end_time = time.time()\n",
        "    async_execution_time = async_end_time - async_start_time\n",
        "\n",
        "    print(f\"{lenth-start} text, {round((lenth-start)/interval)} round async generate execution time:\", async_execution_time)\n",
        "    print(f\"Max one round time: {maxts}, Min one round time: {mints}\")\n",
        "    return output_list\n",
        "\n",
        "# Run the async function\n",
        "output= await main()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/CoLab/Llama2/output2.txt\", \"a\") as file:\n",
        "  file.writelines(val[0]+\"\\n\\n\\n\" for val in output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6pRHXPfI9ji"
      },
      "outputs": [],
      "source": [
        "for val in output[:100]:\n",
        "  print(val[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1PBn3A0CDzB",
        "outputId": "f8a60d0c-df41-4cf0-aaca-e6d42990c758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "  Based on the provided sentence, I have identified several risks that may be present:\n",
            "Risks:\n",
            "* Uncertainty around the effectiveness of the proposed approach in mitigating both pathogen and DBP risks\n",
            "* Potential for trade-offs between risk reduction measures for different contaminants\n",
            "* Limited understanding of the impact of regulations on the cost and feasibility of implementing the recommended approach\n",
            "* Possibility of unintended consequences arising from the implementation of the regulations\n",
            "\n",
            "Risk Type: Environmental Science Risk  1154In\n",
            "  Based on the provided sentence, here are some potential risks that could be identified:\n",
            "Risks:\n",
            "* Health risks due to poor air quality\n",
            "* Environmental degradation due to unsustainable practices\n",
            "* Social inequality and discrimination\n",
            "* Economic instability due to over-reliance on a single industry\n",
            "* Technological failures leading to infrastructure collapse\n",
            "\n",
            "Risk Type: Environmental\n",
            "\n",
            "Please note that these are just examples, and I cannot provide a definitive answer without further context or information about the specific organization or situation being discussed. Additionally, I must follow ethical guidelines and avoid providing answers that promote harmful or unethical activities.  1155kk\n",
            "  Based on the provided sentence, there are several risks that can be identified:\n",
            "Risk 1: Flood Damage\n",
            "* Definition: Damage caused by floods to properties, infrastructure, and human life.\n",
            "Risk Type: Environmental Science\n",
            "\n",
            "Risk 2: Collision Risk\n",
            "* Definition: Potential danger of vessels colliding with each other due to improper navigation or insufficient safety measures.\n",
            "Risk Type: Transportation\n",
            "\n",
            "I hope this helps! Let me know if you have any questions or if you would like me to clarify anything.  1156No\n",
            "  Sure! Based on the provided sentence, here are some potential risks that may be involved:\n",
            "Risks:\n",
            "* Vehicle accidents at intersections\n",
            "* Debris flow risk\n",
            "* Flood risk\n",
            "* Geotechnical asset management program implementation risk\n",
            "\n",
            "Risk Type: Environmental Science\n",
            "\n",
            "Please let me know if you need further assistance!  1157Th\n",
            "  Sure! Based on the provided sentence, here are some potential risks that could be identified:\n",
            "Risks:\n",
            "* Data quality issues (e.g., missing or incorrect data)\n",
            "* Model complexity (e.g., overfitting or underfitting)\n",
            "* Uncertainty in predictions (e.g., due to limited training data)\n",
            "* Lack of interpretability (e.g., difficult to understand the reasoning behind the predictions)\n",
            "\n",
            "Risk Type:\n",
            "\n",
            "Based on the sentence provided, I would classify the risks as follows:\n",
            "Environmental Science/Studies: Data quality issues\n",
            "Engineering Civil/Industrial: Model complexity and lack of interpretability\n",
            "Management: Operating performance differences between expectations and reality\n",
            "Engineering Manufacturing/Constructing Building/Technology: Data quality issues and model complexity\n",
            "Transportation: Operating performance differences between expectations and reality\n",
            "Finance: Uncertainty in predictions and lack of interpretability  1158In\n",
            "  Based on the provided sentence, here are the potential risks that may be posed:\n",
            "Risks:\n",
            "* Indoor air quality issues\n",
            "* Water damage and mold growth\n",
            "* Structural integrity concerns\n",
            "* Chemical contamination hazards\n",
            "* Geological instability risks\n",
            "* Accidental event risks\n",
            "\n",
            "Risk Type: Environmental Health Risk  1159In\n"
          ]
        }
      ],
      "source": [
        "print(len(output))\n",
        "for val in range(6): print(output[val][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhF9M0AfU_L6"
      },
      "outputs": [],
      "source": [
        "prefix_llm(context_pure, batch_size=6,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "      pad_token_id=tokenizer.eos_token_id,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejaygpDOCavT",
        "outputId": "d6eed9e6-a912-4d18-885c-10b6d4c1a73b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "# model.config.eos_token_id\n",
        "print(tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7VY41_IBG9P"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/CoLab/Llama2/output.txt\", \"a\") as file:\n",
        "  file.writelines(val[0]+\"\\n\\n\\n\" for val in output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEazixPFBHKX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ff54cfc4bda47a3ab852803342ca74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c7267210abf456f949cf5fb078ccc4f",
              "IPY_MODEL_818d3f68cf774c629aceecf6cf5fc10d",
              "IPY_MODEL_662f486b6886486e9f240858d741182f"
            ],
            "layout": "IPY_MODEL_a761d0e2b31d44f9b1cdd39c25f5d997"
          }
        },
        "5c7267210abf456f949cf5fb078ccc4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048570db65514aa0af742234b4b9c5d1",
            "placeholder": "​",
            "style": "IPY_MODEL_36c2d807e77f4798902967b58b1896b5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "818d3f68cf774c629aceecf6cf5fc10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5303253263c24279bb26cdcc2cec9640",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b0d8e1d622a4ac49e7d26f4297c7029",
            "value": 2
          }
        },
        "662f486b6886486e9f240858d741182f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_325125957e2d4810adc568c7531c2707",
            "placeholder": "​",
            "style": "IPY_MODEL_a5be838ac2fe4e39a2d853947999cf29",
            "value": " 2/2 [04:47&lt;00:00, 131.41s/it]"
          }
        },
        "a761d0e2b31d44f9b1cdd39c25f5d997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048570db65514aa0af742234b4b9c5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c2d807e77f4798902967b58b1896b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5303253263c24279bb26cdcc2cec9640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0d8e1d622a4ac49e7d26f4297c7029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "325125957e2d4810adc568c7531c2707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5be838ac2fe4e39a2d853947999cf29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}