{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Chrome Download\\Anaconda\\ana\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: d:\\Chrome Download\\Anaconda\\ana\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary d:\\Chrome Download\\Anaconda\\ana\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tsf\n",
    "# ! pip install -qU transformers accelerate langchain==0.0.174 xformers sentencepiece\n",
    "# ! pip install tensorflow\n",
    "import numpy\n",
    "import accelerate\n",
    "import json\n",
    "import transformers\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch, cuda is avaliable: True\n",
      "be remined dont download CPU version: (xxx+cpu) 2.0.1\n",
      "your torch version: 11.7\n",
      "how many GPU you'd have: 1\n",
      " Now show the device number/first device name:  0 NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "print(\"torch, cuda is avaliable:\", torch.cuda.is_available())\n",
    "print(\"be remined dont download CPU version: (xxx+cpu)\", torch.__version__)\n",
    "print(f\"your torch version: {torch.version.cuda}\")\n",
    "device=f\"{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"how many GPU you'd have: {cuda.device_count()}\\n\", \"Now show the device number/first device name: \", device, cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# from huggingfacehub import login\n",
    "straight_path=r\"D:\\LLama2\\llama_hugging\\pytorch_model.bin\"\n",
    "llm=Llama(straight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "output = llm(\"Q: Can you be my wife. A: \", max_tokens=512, stop=[\"Q:\", \"\\n\"], echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"cmpl-b3677b3c-82fa-41bb-a5f9-f0f977176361\",\n",
      "    \"object\": \"text_completion\",\n",
      "    \"created\": 1693063785,\n",
      "    \"model\": \"D:\\\\LLama2\\\\llama_hugging\\\\pytorch_model.bin\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"text\": \"Q: Can you be my wife. A: \\ud83d\\ude36 Sorry, I cannot fulfill that request as I am just a computer program and do not have the ability to form romantic relationships or engage in marriage. My purpose is to assist with tasks and answer questions to the best of my ability based on my training and knowledge. Is there anything else I can help you with?\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 12,\n",
      "        \"completion_tokens\": 69,\n",
      "        \"total_tokens\": 81\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(output, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to use tensorflow frame load model -- this stage may involved to download new model\n",
    "my_store_path=r\"D:\\LLama2\\llama_hugging\"\n",
    "llama_repo=\"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "config = transformers.AutoConfig.from_pretrained(my_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model.embed_tokens\": 0,\n",
      "    \"model.layers.0\": 0,\n",
      "    \"model.layers.1\": 0,\n",
      "    \"model.layers.2\": 0,\n",
      "    \"model.layers.3.self_attn\": 0,\n",
      "    \"model.layers.3.mlp.gate_proj\": 0,\n",
      "    \"model.layers.3.mlp.up_proj\": 0,\n",
      "    \"model.layers.3.mlp.down_proj\": \"cpu\",\n",
      "    \"model.layers.3.mlp.act_fn\": \"cpu\",\n",
      "    \"model.layers.3.input_layernorm\": \"cpu\",\n",
      "    \"model.layers.3.post_attention_layernorm\": \"cpu\",\n",
      "    \"model.layers.4\": \"cpu\",\n",
      "    \"model.layers.5\": \"cpu\",\n",
      "    \"model.layers.6\": \"cpu\",\n",
      "    \"model.layers.7\": \"cpu\",\n",
      "    \"model.layers.8\": \"cpu\",\n",
      "    \"model.layers.9\": \"cpu\",\n",
      "    \"model.layers.10\": \"cpu\",\n",
      "    \"model.layers.12\": \"disk\",\n",
      "    \"model.layers.13\": \"disk\",\n",
      "    \"model.layers.14\": \"disk\",\n",
      "    \"model.layers.15\": \"disk\",\n",
      "    \"model.layers.16\": \"disk\",\n",
      "    \"model.layers.17\": \"disk\",\n",
      "    \"model.layers.18\": \"disk\",\n",
      "    \"model.layers.19\": \"disk\",\n",
      "    \"model.layers.20\": \"disk\",\n",
      "    \"model.layers.21\": \"disk\",\n",
      "    \"model.layers.22\": \"disk\",\n",
      "    \"model.layers.23\": \"disk\",\n",
      "    \"model.layers.24\": \"disk\",\n",
      "    \"model.layers.25\": \"disk\",\n",
      "    \"model.layers.26\": \"disk\",\n",
      "    \"model.layers.27\": \"disk\",\n",
      "    \"model.layers.28\": \"disk\",\n",
      "    \"model.layers.29\": \"disk\",\n",
      "    \"model.layers.30\": \"disk\",\n",
      "    \"model.layers.31\": \"disk\",\n",
      "    \"model.norm\": \"disk\",\n",
      "    \"lm_head\": \"disk\",\n",
      "    \"model.layers.11\": \"disk\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ! conda list torch\n",
    "# print(config)\n",
    "with accelerate.init_empty_weights():\n",
    "    empty_model=transformers.AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "device_map=accelerate.infer_auto_device_map(empty_model, max_memory={0: \"4GiB\", \"cpu\": \"6GiB\"}) # , \n",
    "allocation=json.dumps(device_map, indent=4)\n",
    "print(allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'D:\\LLama2\\llama_hugging\\pytorch_model.bin' at 'D:\\LLama2\\llama_hugging\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\Chrome Download\\Anaconda\\ana\\lib\\site-packages\\transformers\\modeling_utils.py:460\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    461\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Chrome Download\\Anaconda\\ana\\lib\\site-packages\\torch\\serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32md:\\Chrome Download\\Anaconda\\ana\\lib\\site-packages\\torch\\serialization.py:1033\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1029\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1030\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived object of type \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f)\u001b[39m}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1031\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunctionality.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1033\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1034\u001b[0m \u001b[39mif\u001b[39;00m magic_number \u001b[39m!=\u001b[39m MAGIC_NUMBER:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: could not find MARK",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32md:\\Chrome Download\\Anaconda\\ana\\lib\\site-packages\\transformers\\modeling_utils.py:464\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(checkpoint_file) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 464\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39;49mread(\u001b[39m7\u001b[39;49m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    465\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    466\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myou cloned.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m         )\n",
      "File \u001b[1;32md:\\Chrome Download\\Anaconda\\ana\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    321\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[0;32m    323\u001b[0m \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 28: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m=\u001b[39mtransformers\u001b[39m.\u001b[39;49mLlamaForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(my_store_path, device_map\u001b[39m=\u001b[39;49mdevice_map, offload_folder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/tmp/.offload\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      2\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, llm_int8_enable_fp32_cpu_offload\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n",
      "File \u001b[1;32md:\\Chrome Download\\Anaconda\\ana\\lib\\site-packages\\transformers\\modeling_utils.py:2629\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2626\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   2627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2628\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 2629\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   2631\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   2632\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   2633\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   2634\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   2635\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   2636\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Chrome Download\\Anaconda\\ana\\lib\\site-packages\\transformers\\modeling_utils.py:476\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    472\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to locate the file \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m which is necessary to load this pretrained \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mUnicodeDecodeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m--> 476\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    477\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    478\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mat \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'D:\\LLama2\\llama_hugging\\pytorch_model.bin' at 'D:\\LLama2\\llama_hugging\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "model=transformers.LlamaForCausalLM.from_pretrained(my_store_path, device_map=device_map, offload_folder=\"/tmp/.offload\",\n",
    "        load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
